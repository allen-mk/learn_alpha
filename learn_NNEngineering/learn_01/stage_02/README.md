**理解神经网络层（Linear）的本质，并亲手写一个可训练的 MLP。**

我会带你从概念 → 实例 → 代码一步一步搭起来。

---

# 🎯 小节目标

1. 你能理解 `nn.Linear` 到底是什么（数学公式 + 动画般解释）
2. 你能建立一个最简单的两层神经网络
3. 你能用它做一次完整的前向传播（forward）

---

# 🧠 1. 什么是 Linear 层（线性层）？

PyTorch 写法：

```python
layer = torch.nn.Linear(in_features, out_features)
```

数学上做的是：

```
y = x @ W^T + b
```

其中：

* `x` → 输入向量（Tensor）
* `W` → 可学习的权重矩阵
* `b` → 可学习的偏置
* `@` → 矩阵乘法

你可以把它类比为 **游戏引擎中的“数据变换函数”**：
输入一组特征，输出一组新的特征。

---

# 📌 示例：输入是长度为 3 的向量

```
x = [x1, x2, x3]   (3维)

Linear(3, 2)

输出 y = [y1, y2] (2维)
```

计算方式：

```
y1 = x1*w11 + x2*w12 + x3*w13 + b1
y2 = x1*w21 + x2*w22 + x3*w23 + b2
```

**这就是一层神经网络。**

---

# 🎮 2. 前向传播简单示例

我们写一个最小的网络：

* 输入：2 维
* 隐层：4 维
* 输出：1 维（比如用于预测一个分数）

```python
import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(2, 4),   # 输入2 -> 输出4
    nn.ReLU(),         # 激活
    nn.Linear(4, 1)    # 输入4 -> 输出1
)
```

---

# 🌟 3. 运行一次前向传播（forward）

```python
x = torch.tensor([1.0, 2.0])     # 输入
y = model(x)                     # 前向传播
print(y)
```

你会得到这样的输出：

```
tensor([-0.4312], grad_fn=<AddmmBackward0>)
```

> 这就是神经网络生成的新值（根据随机初始化的权重计算出来）。

注意最后的 `grad_fn` 说明它已经构建好计算图了，可以自动求导。

---

# 🔥 解释：为什么要用 `ReLU()`？

因为多个 Linear 层连在一起，如果没有激活函数，会变成一个大号线性层，模型没有表达能力。

**ReLU** 是最常用的激活函数：

```
ReLU(x) = max(0, x)
```

让网络有非线性能力。

---

# 🧪 现在给你一个动手练习（非常简单）

### ✏️ 练习：

请你写出一个神经网络：

* 输入 3 维
* 中间隐藏层 5 维
* 输出 2 维
* 使用 ReLU 激活

并且对随机输入执行一次前向传播。

你只需写代码即可，例如：

```python
# your code
```

我会检查并给你下一步（训练神经网络）。
