
# è¯·ä½ è‡ªå·±å†™ä¸€ä»½è®­ç»ƒä»£ç ï¼Œä»»åŠ¡å¦‚ä¸‹ï¼š
# ğŸ¯ è®­ç»ƒç›®æ ‡
# y = 2*x1 - x2 + 0.5*x3
# è¦æ±‚:
# è¾“å…¥ 3 ç»´
# è¾“å‡º 1 ç»´
# ä½¿ç”¨ MLPï¼ˆè‡³å°‘ 1 ä¸ª ReLUï¼‰
# ä½¿ç”¨ MSELoss
# ä½¿ç”¨ Adam
# è®­ç»ƒ 1000 æ­¥
# æ¯ 200 æ­¥æ‰“å° loss


import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt


model = nn.Sequential(
    nn.Linear(3, 5),
    nn.ReLU(),
    nn.Linear(5, 1)
)

# å®šä¹‰æŸå¤±å‡½æ•°
# loss = (y_pred - y_true)^2
criterion = nn.MSELoss()

# å®šä¹‰ä¼˜åŒ–å™¨
# model.parameters()ï¼šæ‰€æœ‰å¯è®­ç»ƒå‚æ•°ï¼ˆæƒé‡ + åç½®ï¼‰
# Adam : æœ€å¸¸ç”¨ã€æœ€ç¨³å®šçš„ä¼˜åŒ–å™¨
# lr : å­¦ä¹ ç‡(æ­¥å­å¤§å°)
optimizer = optim.Adam(model.parameters(), lr=0.01)

COUNT = 10000

for step in range(COUNT):
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    x = torch.rand((64, 3))                # batch size = 64

    y_true = (2*x[:, 0] - x[:, 1] + 0.5*x[:, 2]).view(-1, 1)

    y_pred = model(x)

    loss = criterion(y_pred, y_true)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if step % 200 == 0:
        print(f"step {step}, loss = {loss.item():.8f}")

for p in model.parameters():
    print(p)
    